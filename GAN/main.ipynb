{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    print(\"Using GPU\")\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_visible_devices(gpu_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data'\n",
    "image_size = (32, 32)\n",
    "batch_size = 256\n",
    "\n",
    "def custom_preprocess(x):\n",
    "    x = (x - 127.5) / 127.5\n",
    "    return x\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=custom_preprocess, # rescale [0.0 - 255.0] to [-1.0 to 1.0] range\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "train_data = image_generator.flow_from_directory(\n",
    "    image_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,  # No labels needed for GAN training\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_data,\n",
    "    output_signature=tf.TensorSpec(shape=(None, image_size[0], image_size[1], 3), dtype=tf.float32) #RGB images so 3 channels, float32 for GPU acceleration\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_h, n_c):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_h = n_h\n",
    "        self.n_c = n_c  # number of chanells, I want to generate RGB so it's  set to 3\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(256 * 4 * 4)\n",
    "        self.reshape = tf.keras.layers.Reshape((4, 4, 256))\n",
    "        self.upsample1 = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.conv1 = tf.keras.layers.Conv2DTranspose(128, kernel_size=5, padding='same')\n",
    "        self.upsample2 = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.conv2 = tf.keras.layers.Conv2DTranspose(64, kernel_size=5, padding='same')\n",
    "        self.upsample3 = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.conv3 = tf.keras.layers.Conv2DTranspose(32, kernel_size=5, padding='same')\n",
    "        self.conv4 = tf.keras.layers.Conv2DTranspose(n_c, kernel_size=1, padding='same')\n",
    "\n",
    "    def call(self, Z):\n",
    "        h = tf.nn.leaky_relu(self.dense1(Z), 0.2)\n",
    "        h = self.reshape(h)\n",
    "        h = self.upsample1(h)\n",
    "        h = tf.nn.leaky_relu(self.conv1(h), 0.2)\n",
    "        h = self.upsample2(h)\n",
    "        h = tf.nn.leaky_relu(self.conv2(h), 0.2)\n",
    "        h = self.upsample3(h)\n",
    "        h = tf.nn.leaky_relu(self.conv3(h), 0.2)\n",
    "        x = self.conv4(h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, X):\n",
    "        h = tf.nn.leaky_relu(self.conv1(X))\n",
    "        h = tf.nn.leaky_relu(self.conv2(h))\n",
    "        h = tf.nn.leaky_relu(self.conv3(h))\n",
    "        h = self.flatten(h)\n",
    "        logits  = self.dense1(h)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "def get_noise(n_batch, noise_var):\n",
    "    return np.random.randn(n_batch, noise_var).astype(np.float32)\n",
    "\n",
    "def visualize_images(images, n_rows, n_cols, title=None):\n",
    "    images = images / 2 + 0.5 # Rescale to [0, 1] range\n",
    "    images = np.clip(images, 0, 1)\n",
    "    _, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols, n_rows))\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            ax = axes[i, j]\n",
    "            ax.imshow(images[i * n_cols + j])\n",
    "            ax.axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve(g_loss_history, d_g_z_loss_history, d_x_loss_history, d_loss_history):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(g_loss_history, label='Generator Loss', alpha=0.7)\n",
    "    plt.plot(d_g_z_loss_history, label='Discriminator (Generated) Loss', alpha=0.7)\n",
    "    plt.plot(d_x_loss_history, label='Discriminator (Real) Loss', alpha=0.7)\n",
    "    plt.plot(d_loss_history, label='Discriminator Total Loss', alpha=0.7)\n",
    "    plt.xlabel('Number of iterations in hundreds')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Learning Curves')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters for GAN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h, n_w, n_c = [32, 32, 3]\n",
    "latent_size = 128\n",
    "n_batch = 100\n",
    "lr = 1e-4\n",
    "n_updates_total = 25000\n",
    "noise_vector = get_noise(n_batch, latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.layers.Input(shape=(n_h, n_w, n_c), dtype=tf.float32)\n",
    "Z = tf.keras.layers.Input(shape=(latent_size,), dtype=tf.float32)\n",
    "\n",
    "generator = Generator(n_h=n_h, n_c=n_c)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "g_z = generator(Z)\n",
    "d_g_z = discriminator(g_z)\n",
    "d_x = discriminator(X)\n",
    "\n",
    "g_loss = cross_entropy_loss(logits=d_g_z, labels=tf.ones(tf.shape(d_g_z)))\n",
    "d_g_z_loss = cross_entropy_loss(logits=d_g_z, labels=tf.zeros(tf.shape(d_g_z)))\n",
    "d_x_loss = cross_entropy_loss(logits=d_x, labels=tf.ones(tf.shape(d_x)))\n",
    "d_loss = (d_g_z_loss + d_x_loss) / 2\n",
    "\n",
    "g_vars = generator.trainable_variables\n",
    "d_vars = discriminator.trainable_variables\n",
    "\n",
    "g_optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=0.5)\n",
    "d_optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=0.5)\n",
    "\n",
    "g_loss_history = []\n",
    "d_g_z_loss_history = []\n",
    "d_x_loss_history = []\n",
    "d_loss_history = []\n",
    "\n",
    "for n_updates in tqdm(range(n_updates_total), ncols=80, leave=False):\n",
    "    xs = train_data.next()\n",
    "    noise_samples = get_noise(n_batch, latent_size)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        d_g_z = discriminator(generator(noise_samples))\n",
    "        d_x = discriminator(xs)\n",
    "        \n",
    "        d_g_z_loss = cross_entropy_loss(logits=d_g_z, labels=tf.zeros_like(d_g_z))\n",
    "        d_x_loss = cross_entropy_loss(logits=d_x, labels=tf.ones_like(d_x))\n",
    "        d_loss = (d_g_z_loss + d_x_loss) / 2\n",
    "    \n",
    "    d_gradients = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "    \n",
    "    # Training steps for the generator\n",
    "    with tf.GradientTape() as tape:\n",
    "        g_z = generator(noise_samples)\n",
    "        d_g_z = discriminator(g_z)\n",
    "        g_loss = cross_entropy_loss(logits=d_g_z, labels=tf.ones_like(d_g_z))\n",
    "    \n",
    "    g_gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "\n",
    "    if n_updates % 100 == 0:\n",
    "        g_loss_history.append(g_loss)\n",
    "        d_g_z_loss_history.append(d_g_z_loss)\n",
    "        d_x_loss_history.append(d_x_loss)\n",
    "        d_loss_history.append(d_loss)\n",
    "    \n",
    "    if n_updates % 1000 == 0:\n",
    "        generated_images = generator(noise_vector)\n",
    "        generated_images = generated_images / 2 + 0.5 # Rescale to [0, 1] range\n",
    "        generated_images = np.clip(generated_images, 0, 1)\n",
    "        n_rows = 10\n",
    "        n_cols = 10\n",
    "        visualize_images(generated_images, n_rows, n_cols, title=f\"Generated images after {n_updates} iterations\")\n",
    "        plot_learning_curve(g_loss_history, d_g_z_loss_history, d_x_loss_history, d_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Generator and Discriminator models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_weights = generator.get_weights()\n",
    "with open('generator_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(generator_weights, f)\n",
    "\n",
    "discriminator_weights = discriminator.get_weights()\n",
    "with open('discriminator_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(discriminator_weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = '../data'\n",
    "image_size = (32, 32)\n",
    "n_h, n_w, n_c = [32, 32, 3]\n",
    "latent_size = 128\n",
    "n_updates_total = 10000\n",
    "\n",
    "model_counter = 0\n",
    "for batch_size in [128, 256, 512]:\n",
    "    for lr in [1e-4, 1e-5, 1e-6]:\n",
    "        for latent_size in [256, 512]:\n",
    "            for n_batch in [100, 196]:\n",
    "                noise_vector = get_noise(n_batch, latent_size)\n",
    "\n",
    "                def custom_preprocess(x):\n",
    "                    x = (x - 127.5) / 127.5\n",
    "                    return x\n",
    "\n",
    "                image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                    preprocessing_function=custom_preprocess, # rescale [0.0 - 255.0] to [-1.0 to 1.0] range\n",
    "                    validation_split=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                )\n",
    "\n",
    "                train_data = image_generator.flow_from_directory(\n",
    "                    image_dir,\n",
    "                    target_size=image_size,\n",
    "                    batch_size=batch_size,\n",
    "                    class_mode=None,  # No labels needed for GAN training\n",
    "                    subset='training'\n",
    "                )\n",
    "\n",
    "                train_dataset = tf.data.Dataset.from_generator(\n",
    "                    lambda: train_data,\n",
    "                    output_signature=tf.TensorSpec(shape=(None, image_size[0], image_size[1], 3), dtype=tf.float32) #RGB images so 3 channels, float32 for GPU acceleration\n",
    "                )\n",
    "\n",
    "                train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "                train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "                X = tf.keras.layers.Input(shape=(n_h, n_w, n_c), dtype=tf.float32)\n",
    "                Z = tf.keras.layers.Input(shape=(latent_size,), dtype=tf.float32)\n",
    "\n",
    "                generator = Generator(n_h=n_h, n_c=n_c)\n",
    "                discriminator = Discriminator()\n",
    "\n",
    "                g_z = generator(Z)\n",
    "                d_g_z = discriminator(g_z)\n",
    "                d_x = discriminator(X)\n",
    "\n",
    "                g_loss = cross_entropy_loss(logits=d_g_z, labels=tf.ones(tf.shape(d_g_z)))\n",
    "                d_g_z_loss = cross_entropy_loss(logits=d_g_z, labels=tf.zeros(tf.shape(d_g_z)))\n",
    "                d_x_loss = cross_entropy_loss(logits=d_x, labels=tf.ones(tf.shape(d_x)))\n",
    "                d_loss = (d_g_z_loss + d_x_loss) / 2\n",
    "\n",
    "                g_vars = generator.trainable_variables\n",
    "                d_vars = discriminator.trainable_variables\n",
    "\n",
    "                g_optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=0.5)\n",
    "                d_optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=0.5)\n",
    "\n",
    "                g_loss_history = []\n",
    "                d_g_z_loss_history = []\n",
    "                d_x_loss_history = []\n",
    "                d_loss_history = []\n",
    "\n",
    "                for n_updates in tqdm(range(n_updates_total), ncols=80, leave=False):\n",
    "                    xs = train_data.next()\n",
    "                    noise_samples = get_noise(n_batch, latent_size)\n",
    "                    \n",
    "                    with tf.GradientTape() as tape:\n",
    "                        d_g_z = discriminator(generator(noise_samples))\n",
    "                        d_x = discriminator(xs)\n",
    "                        \n",
    "                        d_g_z_loss = cross_entropy_loss(logits=d_g_z, labels=tf.zeros_like(d_g_z))\n",
    "                        d_x_loss = cross_entropy_loss(logits=d_x, labels=tf.ones_like(d_x))\n",
    "                        d_loss = (d_g_z_loss + d_x_loss) / 2\n",
    "                    \n",
    "                    d_gradients = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "                    d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "                    \n",
    "                    # Training steps for the generator\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        g_z = generator(noise_samples)\n",
    "                        d_g_z = discriminator(g_z)\n",
    "                        g_loss = cross_entropy_loss(logits=d_g_z, labels=tf.ones_like(d_g_z))\n",
    "                    \n",
    "                    g_gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
    "                    g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "\n",
    "                    if n_updates % 100 == 0:\n",
    "                        g_loss_history.append(g_loss)\n",
    "                        d_g_z_loss_history.append(d_g_z_loss)\n",
    "                        d_x_loss_history.append(d_x_loss)\n",
    "                        d_loss_history.append(d_loss)\n",
    "                    \n",
    "                    if n_updates % 2500 == 0:\n",
    "                        generated_images = generator(noise_vector)\n",
    "                        generated_images = generated_images / 2 + 0.5 # Rescale to [0, 1] range\n",
    "                        generated_images = np.clip(generated_images, 0, 1)\n",
    "                        n_rows = n_cols = int(np.sqrt(n_batch))\n",
    "                        visualize_images(generated_images, n_rows, n_cols, title=f\"Generated images after {n_updates} iterations\")\n",
    "                        plot_learning_curve(g_loss_history, d_g_z_loss_history, d_x_loss_history, d_loss_history)\n",
    "                \n",
    "                model_counter += 1\n",
    "                generator_weights = generator.get_weights()\n",
    "                with open(f'models/generator_weights_{model_counter}.pkl', 'wb') as f:\n",
    "                    pickle.dump(generator_weights, f)\n",
    "\n",
    "                discriminator_weights = discriminator.get_weights()\n",
    "                with open(f'models/discriminator_weights_{model_counter}.pkl', 'wb') as f:\n",
    "                    pickle.dump(discriminator_weights, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative-Modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
