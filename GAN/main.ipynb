{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Layer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data'\n",
    "image_size = (32, 32)\n",
    "batch_size = 54\n",
    "\n",
    "def custom_preprocess(x):\n",
    "    x = (x - 127.5) / 127.5\n",
    "    return x\n",
    "\n",
    "image_generator = ImageDataGenerator(\n",
    "    preprocessing_function=custom_preprocess, # rescale [0.0 - 255.0] to [-1.0 to 1.0] range\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "train_data = image_generator.flow_from_directory(\n",
    "    image_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,  # No labels needed for GAN training\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_data,\n",
    "    output_signature=tf.TensorSpec(shape=(None, image_size[0], image_size[1], 3), dtype=tf.float32),\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Layer):\n",
    "    def __init__(self, n_hidden, n_c):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_c = n_c  # number of chanells, I want to generate RGB so it's  set to 3\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(256 * 4 * 4)\n",
    "        self.reshape = tf.keras.layers.Reshape((4, 4, 256))\n",
    "        self.upsample1 = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.conv1 = tf.keras.layers.Conv2DTranspose(128, kernel_size=5, padding='same')\n",
    "        self.upsample2 = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.conv2 = tf.keras.layers.Conv2DTranspose(64, kernel_size=5, padding='same')\n",
    "        self.upsample3 = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.conv3 = tf.keras.layers.Conv2DTranspose(32, kernel_size=5, padding='same')\n",
    "        self.conv4 = tf.keras.layers.Conv2DTranspose(n_c, kernel_size=1, padding='same')\n",
    "\n",
    "    def call(self, Z):\n",
    "        h = tf.nn.leaky_relu(self.dense1(Z), 0.2)\n",
    "        h = self.reshape(h)\n",
    "        h = self.upsample1(h)\n",
    "        h = tf.nn.leaky_relu(self.conv1(h), 0.2)\n",
    "        h = self.upsample2(h)\n",
    "        h = tf.nn.leaky_relu(self.conv2(h), 0.2)\n",
    "        h = self.upsample3(h)\n",
    "        h = tf.nn.leaky_relu(self.conv3(h), 0.2)\n",
    "        x = self.conv4(h)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, X):\n",
    "        h = tf.nn.leaky_relu(self.conv1(X))\n",
    "        h = tf.nn.leaky_relu(self.conv2(h))\n",
    "        h = tf.nn.leaky_relu(self.conv3(h))\n",
    "        h = self.flatten(h)\n",
    "        logits  = self.dense1(h)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "def get_noise(n_batch, noise_var):\n",
    "    return np.random.randn(n_batch, noise_var).astype(np.float32)\n",
    "\n",
    "def visualize_images(images, n_rows, n_cols, title=None):\n",
    "    _, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            ax = axes[i, j]\n",
    "            ax.imshow(images[i * n_cols + j])\n",
    "            ax.axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve(g_loss_history, d_g_z_loss_history, d_x_loss_history, d_loss_history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(g_loss_history, label='Generator Loss', alpha=0.7)\n",
    "    plt.plot(d_g_z_loss_history, label='Discriminator (Generated) Loss', alpha=0.7)\n",
    "    plt.plot(d_x_loss_history, label='Discriminator (Real) Loss', alpha=0.7)\n",
    "    plt.plot(d_loss_history, label='Discriminator Total Loss', alpha=0.7)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Learning Curves')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters for GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden, n_w, n_c = [32, 32, 3]\n",
    "noise_var = 128\n",
    "n_batch = 100\n",
    "lr = 1e-4\n",
    "n_updates_total = 55000\n",
    "noise_vector = get_noise(n_batch, noise_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.layers.Input(shape=(n_hidden, n_w, n_c), dtype=tf.float32)\n",
    "Z = tf.keras.layers.Input(shape=(noise_var,), dtype=tf.float32)\n",
    "\n",
    "generator = Generator(n_hidden=n_hidden, n_c=n_c)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "g_z = generator(Z)\n",
    "d_g_z = discriminator(g_z)\n",
    "d_x = discriminator(X)\n",
    "\n",
    "g_loss = cross_entropy_loss(logits=d_g_z, labels=tf.ones(tf.shape(d_g_z)))\n",
    "d_g_z_loss = cross_entropy_loss(logits=d_g_z, labels=tf.zeros(tf.shape(d_g_z)))\n",
    "d_x_loss = cross_entropy_loss(logits=d_x, labels=tf.ones(tf.shape(d_x)))\n",
    "d_loss = (d_g_z_loss + d_x_loss) / 2\n",
    "\n",
    "g_vars = generator.trainable_variables\n",
    "d_vars = discriminator.trainable_variables\n",
    "\n",
    "g_optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=0.5)\n",
    "d_optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=0.5)\n",
    "\n",
    "g_loss_history = []\n",
    "d_g_z_loss_history = []\n",
    "d_x_loss_history = []\n",
    "d_loss_history = []\n",
    "\n",
    "for n_updates in tqdm(range(n_updates_total), ncols=80, leave=False):\n",
    "    xs = train_data.next()\n",
    "    noise_samples = get_noise(n_batch, noise_var)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        d_g_z = discriminator(generator(noise_samples))\n",
    "        d_x = discriminator(xs)\n",
    "        \n",
    "        d_g_z_loss = cross_entropy_loss(logits=d_g_z, labels=tf.zeros_like(d_g_z))\n",
    "        d_x_loss = cross_entropy_loss(logits=d_x, labels=tf.ones_like(d_x))\n",
    "        d_loss = (d_g_z_loss + d_x_loss) / 2\n",
    "    \n",
    "    d_gradients = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "    \n",
    "    # Training steps for the generator\n",
    "    with tf.GradientTape() as tape:\n",
    "        g_z = generator(noise_samples)\n",
    "        d_g_z = discriminator(g_z)\n",
    "        g_loss = cross_entropy_loss(logits=d_g_z, labels=tf.ones_like(d_g_z))\n",
    "    \n",
    "    g_gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "\n",
    "    if n_updates % 100 == 0:\n",
    "        g_loss_history.append(g_loss)\n",
    "        d_g_z_loss_history.append(d_g_z_loss)\n",
    "        d_x_loss_history.append(d_x_loss)\n",
    "        d_loss_history.append(d_loss)\n",
    "    \n",
    "    if n_updates % 1000 == 0:\n",
    "        generated_images = generator(noise_vector)\n",
    "        generated_images = generated_images / 2 + 0.5 # Rescale to [0, 1] range\n",
    "        generated_images = np.clip(generated_images, 0, 1)\n",
    "        n_rows = 10\n",
    "        n_cols = 10\n",
    "        visualize_images(generated_images, n_rows, n_cols, title=f\"Generated images after {n_updates} iterations\")\n",
    "        plot_learning_curve(g_loss_history, d_g_z_loss_history, d_x_loss_history, d_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = generator(noise_vector)\n",
    "generated_images = generated_images / 2 + 0.5 # Rescale to [0, 1] range\n",
    "n_rows = 10\n",
    "n_cols = 10\n",
    "visualize_images(generated_images, n_rows, n_cols, title=f\"Generated images after {n_updates} iterations\")\n",
    "plot_learning_curve(g_loss_history, d_g_z_loss_history, d_x_loss_history, d_loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative-Modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
