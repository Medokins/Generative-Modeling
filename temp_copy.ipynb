{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    print(\"Using GPU\")\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_visible_devices(gpu_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data'\n",
    "image_size = (32, 32)\n",
    "batch_size = 256\n",
    "\n",
    "def custom_preprocess(x):\n",
    "    x = (x - 127.5) / 127.5\n",
    "    return x\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=custom_preprocess, # rescale [0.0 - 255.0] to [-1.0 to 1.0] range\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "train_data = image_generator.flow_from_directory(\n",
    "    image_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,  # No labels needed for GAN training\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_data,\n",
    "    output_signature=tf.TensorSpec(shape=(None, image_size[0], image_size[1], 3), dtype=tf.float32) #RGB images so 3 channels, float32 for GPU acceleration\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Generator, self).__init__()\n",
    "        input_layer = tf.keras.layers.Input(shape=(latent_size, ), dtype=tf.float32)\n",
    "\n",
    "        dense_layer = tf.keras.layers.Dense(256 * 4 * 4)(input_layer)\n",
    "        leaky_relu_1 = tf.keras.layers.LeakyReLU(0.2)(dense_layer)\n",
    "        reshape_1 = tf.keras.layers.Reshape((4, 4, 256))(leaky_relu_1)\n",
    "        upsample_1 = tf.keras.layers.UpSampling2D(size=(2, 2))(reshape_1)\n",
    "\n",
    "        conv_1 = tf.keras.layers.Conv2DTranspose(256, kernel_size=5, padding=\"same\")(upsample_1)\n",
    "        leaky_relu_2 = tf.keras.layers.LeakyReLU(0.2)(conv_1)\n",
    "        upsample_2 = tf.keras.layers.UpSampling2D(size=(2, 2))(leaky_relu_2)\n",
    "\n",
    "        conv_2 = tf.keras.layers.Conv2DTranspose(128, kernel_size=5, padding=\"same\")(upsample_2)\n",
    "        leaky_relu_3 = tf.keras.layers.LeakyReLU(0.2)(conv_2)\n",
    "        upsample_3 = tf.keras.layers.UpSampling2D(size=(2, 2))(leaky_relu_3)\n",
    "\n",
    "        conv_3 = tf.keras.layers.Conv2DTranspose(32, kernel_size=5, padding=\"same\")(upsample_3)\n",
    "        leaky_relu_4 = tf.keras.layers.LeakyReLU(0.2)(conv_3)\n",
    "\n",
    "        output_layer = tf.keras.layers.Conv2DTranspose(3, kernel_size=1, padding='same')(leaky_relu_4)\n",
    "        \n",
    "        self.generator = tf.keras.Model(input_layer, output_layer, name='generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, image_size): \n",
    "        super(Discriminator, self).__init__()\n",
    "        input_layer = tf.keras.layers.Input(shape=(image_size[0], image_size[1], 3), dtype=tf.float32)\n",
    "\n",
    "        conv_1 = tf.keras.layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same')(input_layer)\n",
    "        leaky_relu_1 = tf.keras.layers.LeakyReLU(0.2)(conv_1)\n",
    "        conv_2 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(leaky_relu_1)\n",
    "        leaky_relu_2 = tf.keras.layers.LeakyReLU(0.2)(conv_2)\n",
    "        conv_3 = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(leaky_relu_2)\n",
    "        leaky_relu_3 = tf.keras.layers.LeakyReLU(0.2)(conv_3)\n",
    "        flatten = tf.keras.layers.Flatten()(leaky_relu_3)\n",
    "        output_layer = tf.keras.layers.Dense(1)(flatten)\n",
    "\n",
    "        self.discriminator = tf.keras.Model(input_layer, output_layer, name='discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "def get_noise(n_batch, noise_var):\n",
    "    return np.random.randn(n_batch, noise_var).astype(np.float32)\n",
    "\n",
    "def visualize_images(images, n_rows, n_cols, title=None):\n",
    "    images = images / 2 + 0.5 # Rescale to [0, 1] range\n",
    "    images = np.clip(images, 0, 1)\n",
    "    _, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols, n_rows))\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            ax = axes[i, j]\n",
    "            ax.imshow(images[i * n_cols + j])\n",
    "            ax.axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve(g_loss_history, d_g_z_loss_history, d_x_loss_history, d_loss_history):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(g_loss_history, label='Generator Loss', alpha=0.7)\n",
    "    plt.plot(d_g_z_loss_history, label='Discriminator (Generated) Loss', alpha=0.7)\n",
    "    plt.plot(d_x_loss_history, label='Discriminator (Real) Loss', alpha=0.7)\n",
    "    plt.plot(d_loss_history, label='Discriminator Total Loss', alpha=0.7)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Learning Curves')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters for GAN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 128\n",
    "n_batch = 100  # num of generated images per noise vector\n",
    "lr = 1e-5\n",
    "n_updates_total = 40000\n",
    "noise_vector = get_noise(n_batch, latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_size=latent_size)\n",
    "discriminator = Discriminator(image_size=image_size)\n",
    "\n",
    "g_z = generator.generator(noise_vector)\n",
    "d_g_z = discriminator.discriminator(g_z)\n",
    "d_x = discriminator.discriminator(train_data.next())\n",
    "\n",
    "g_loss = cross_entropy_loss(logits=d_g_z, labels=tf.ones(tf.shape(d_g_z)))\n",
    "d_g_z_loss = cross_entropy_loss(logits=d_g_z, labels=tf.zeros(tf.shape(d_g_z)))\n",
    "d_x_loss = cross_entropy_loss(logits=d_x, labels=tf.ones(tf.shape(d_x)))\n",
    "d_loss = (d_g_z_loss + d_x_loss) / 2\n",
    "\n",
    "g_vars = generator.trainable_variables\n",
    "d_vars = discriminator.trainable_variables\n",
    "\n",
    "g_optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=0.5)\n",
    "d_optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=0.5)\n",
    "\n",
    "g_loss_history = []\n",
    "d_g_z_loss_history = []\n",
    "d_x_loss_history = []\n",
    "d_loss_history = []\n",
    "\n",
    "for n_updates in tqdm(range(n_updates_total), ncols=80, leave=False):\n",
    "    xs = train_data.next()\n",
    "    noise_samples = get_noise(n_batch, latent_size)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        d_g_z = discriminator.discriminator(generator.generator(noise_samples))\n",
    "        d_x = discriminator.discriminator(xs)\n",
    "        \n",
    "        d_g_z_loss = cross_entropy_loss(logits=d_g_z, labels=tf.zeros_like(d_g_z))\n",
    "        d_x_loss = cross_entropy_loss(logits=d_x, labels=tf.ones_like(d_x))\n",
    "        d_loss = (d_g_z_loss + d_x_loss) / 2\n",
    "    \n",
    "    d_gradients = tape.gradient(d_loss, discriminator.discriminator.trainable_variables)\n",
    "    d_optimizer.apply_gradients(zip(d_gradients, discriminator.discriminator.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        g_z = generator.generator(noise_samples)\n",
    "        d_g_z = discriminator.discriminator(g_z)\n",
    "        g_loss = cross_entropy_loss(logits=d_g_z, labels=tf.ones_like(d_g_z))\n",
    "    \n",
    "    g_gradients = tape.gradient(g_loss, generator.generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(g_gradients, generator.generator.trainable_variables))\n",
    "\n",
    "    if n_updates % 100 == 0:\n",
    "        g_loss_history.append(g_loss)\n",
    "        d_g_z_loss_history.append(d_g_z_loss)\n",
    "        d_x_loss_history.append(d_x_loss)\n",
    "        d_loss_history.append(d_loss)\n",
    "    \n",
    "    if n_updates % 1000 == 0:\n",
    "        generated_images = generator.generator(noise_vector)\n",
    "        n_rows = 10\n",
    "        n_cols = 10\n",
    "        visualize_images(generated_images, n_rows, n_cols, title=f\"Generated images after {n_updates} iterations\")\n",
    "        plot_learning_curve(g_loss_history, d_g_z_loss_history, d_x_loss_history, d_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Generator and Discriminator models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_weights = generator.get_weights()\n",
    "# with open('generator_weights.pkl', 'wb') as f:\n",
    "#     pickle.dump(generator_weights, f)\n",
    "\n",
    "# discriminator_weights = discriminator.get_weights()\n",
    "# with open('discriminator_weights.pkl', 'wb') as f:\n",
    "#     pickle.dump(discriminator_weights, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative-Modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
